# -*- coding: utf-8 -*-
"""Augment Sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vyehoqrn_fmrknyTK63ToV_ouep0EfKj
"""

from google.colab import files
from google.colab import drive
#drive.mount('/content/drive')



files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000 -p /content/skin/ --force

!unzip /content/skin/\*.zip -d /content/skin/

import glob
import os
from tqdm import tqdm
import os, cv2,itertools
import pandas as pd

from glob import glob
from PIL import Image
import matplotlib.pyplot as plt

from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix
import itertools

import keras
from keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras import backend as K
import itertools
from keras.layers.normalization import BatchNormalization
from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras import applications


from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd



def plot_model_history(model_history):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    # summarize history for accuracy
    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])
    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)
    axs[0].legend(['train', 'val'], loc='best')
    # summarize history for loss
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['train', 'val'], loc='best')
    plt.show()

def create_confusion_matrix(Ypredicted,YtestGroundTruth):
    classes=['akiec', 'bcc', 'bkl', 'df', 'nv', 'mel','vasc']
    y_pred=Ypredicted
    con_mat = tf.math.confusion_matrix(labels=YtestGroundTruth, predictions=y_pred)
    con_mat_norm1 = con_mat/con_mat.numpy().sum(axis=1)[:,tf.newaxis]
    con_mat_norm = np.around(con_mat_norm1, decimals=2)
    con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)
#figure = plt.figure(figsize=(8, 8))
    plt.figure(figsize=(8, 8))
    sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()
    plotFile = sys.argv[0].split('/')[-1]
    plt.savefig(plotFile+'_confusionMatrix.png')

tile_df = pd.read_csv('/content/skin/HAM10000_metadata.csv')

folders = glob(os.path.join('skin','*'))
folders
imageid_path_dict = {}
for folder in folders:
  imageid_path_dict.update({os.path.splitext(os.path.basename(x))[0]: x
                     for x in glob(os.path.join(folder,'*.jpg'))})

lesion_type_dict = {
    'nv': 'Melanocytic nevi',
    'mel': 'Melanoma',
    'bkl': 'Benign keratosis-like lesions ',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}

tile_df['path'] = tile_df['image_id'].map(imageid_path_dict.get)
tile_df['cell_type'] = tile_df['dx'].map(lesion_type_dict.get)
tile_df['cell_type_idx'] = pd.Categorical(tile_df['cell_type']).codes
tile_df.sample(3)

tile_df.describe(exclude=[np.number])

from skimage.io import imread
from PIL import Image as pil_image
## Resize images
tile_df['image'] = tile_df['path'].map(lambda x: np.asarray(pil_image.open(x).resize((224,224))))

df_undup = tile_df.groupby("lesion_id").count()
df_undup = df_undup[df_undup['image_id'] == 1]
df_undup.reset_index(inplace=True)
df_undup.head()

def get_duplicates(x):
    unique_list = list(df_undup['lesion_id'])
    if x in unique_list:
        return 'unduplicated'
    else:
        return 'duplicated'
    
tile_df['duplicates'] = tile_df['lesion_id']
tile_df['duplicates'] = tile_df['duplicates'].apply(get_duplicates)

tile_df['duplicates'].value_counts()

### Image don't have duplicates

df_undup = tile_df[tile_df['duplicates'] == 'unduplicated']
df_undup.shape

### now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set
y = df_undup['cell_type_idx']
_, df_val = train_test_split(df_undup, test_size=0.4, random_state=101, stratify=y)
df_val.shape

def get_val_rows(x):
    # create a list of all the lesion_id's in the val set
    val_list = list(df_val['image_id'])
    if str(x) in val_list:
        return 'val'
    else:
        return 'train'
    
tile_df['train_or_val'] = tile_df['image_id']
tile_df['train_or_val'] = tile_df['train_or_val'].apply(get_val_rows)

df_train0 = tile_df[tile_df['train_or_val'] == 'train']
print(len(df_train0))
print(len(df_val))

df_val['cell_type'].value_counts()

## Equalization sampling
df_train = df_train0
data_aug_rate = [15,10,5,50,0,5,40]
for i in range(7):
    if data_aug_rate[i]:
        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)
        
df_train['cell_type'].value_counts()

df_val, df_test = train_test_split(df_val, test_size=0.5)
df_train = df_train0.reset_index()
df_val = df_val.reset_index()
df_test = df_test.reset_index()

x_train = np.asarray(df_train['image'].tolist())
x_test = np.asarray(df_test['image'].tolist())
x_val = np.asarray(df_val['image'].tolist())

y_train = to_categorical(df_train["cell_type_idx"], num_classes = 7)
y_test = to_categorical(df_test["cell_type_idx"], num_classes = 7)
y_val = to_categorical(df_val["cell_type_idx"],num_classes = 7)



x_train = x_train.reshape(x_train.shape[0], *(224, 224, 3)).astype('float32')
x_test = x_test.reshape(x_test.shape[0], *(224, 224, 3)).astype('float32')
x_val = x_val.reshape(x_val.shape[0], *(224, 224, 3)).astype('float32')
x_train = x_train/255.0
x_val = x_val/255.0
x_test = x_test/255.0

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False, 
        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images

datagen.fit(x_train)

"""## Inception V3

"""



from keras.applications.inception_v3 import InceptionV3
# load model
model3 = Sequential()
model3.add(InceptionV3(include_top = False,input_shape = (224,224,3),pooling = 'avg',weights = 'imagenet'))
model3.add(Flatten())
model3.add(Dense(1024, activation='relu'))
model3.add(Dropout(0.5))
model3.add(BatchNormalization())
model3.add(Dense(7, activation = "softmax"))
model3.summary()
batch_size = 16
epoch = 30
model3.compile(optimizer=keras.optimizers.RMSprop(lr = 2e-5),loss='categorical_crossentropy',metrics=['accuracy'])

from keras.callbacks import EarlyStopping, ModelCheckpoint

cb_early_stopper = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=50)
cb_checkpointer = ModelCheckpoint(filepath = '../working/best.hdf5', monitor = 'val_acc', mode='max', save_best_only = True)

history = model3.fit(datagen.flow(x_train,y_train, batch_size = batch_size), epochs = epoch,
                              validation_data = (x_val, y_val), verbose = 1,
                                  steps_per_epoch=x_train.shape[0] // batch_size,
                                  callbacks=[cb_checkpointer])



_, test_acc = model3.evaluate(x_test, y_test, verbose=0)

test_acc

plot_model_history(history)

import tensorflow as tf
import seaborn as sns
import sys

plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'mel','vasc']
Ypredicted=model3.predict(x_test)
Ypredicted = np.argmax(Ypredicted, axis = 1)
Y_true = np.argmax(y_test,axis = 1) 
create_confusion_matrix(Ypredicted,Y_true)

test_pred = model3.predict(x_test)
# Convert predictions classes to one hot vectors 
test_pred_classes = np.argmax(test_pred,axis = 1) 
test_pred_classes = to_categorical(test_pred_classes, num_classes = 7)

from sklearn.metrics import classification_report
# Generate a classification report
testreport = classification_report(y_test, test_pred_classes, target_names=plot_labels)

print(testreport)

"""## ResNet50"""

from keras.applications.resnet50 import ResNet50

model = Sequential()
model.add(ResNet50(include_top = False,input_shape = (224,224,3),pooling = 'avg',weights = 'imagenet'))
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(7, activation = "softmax"))
model.summary()
batch_size = 16
epoch = 30
model.compile(optimizer=keras.optimizers.RMSprop(lr = 2e-5),loss='binary_crossentropy',metrics=['accuracy'])

from keras.callbacks import EarlyStopping, ModelCheckpoint

cb_early_stopper = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=50)
cb_checkpointer = ModelCheckpoint(filepath = '../working/best.hdf5', monitor = 'val_acc', mode='max', save_best_only = True)

history = model.fit(datagen.flow(x_train,y_train, batch_size = batch_size), epochs = epoch,
                              validation_data = (x_val, y_val), verbose = 1,
                                  steps_per_epoch=x_train.shape[0] // batch_size,
                                  callbacks=[cb_checkpointer])



_, test_acc = model.evaluate(x_test, y_test, verbose=0)

test_acc

plot_model_history(history)



import tensorflow as tf
import seaborn as sns
import sys

plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'mel','vasc']
Ypredicted=model.predict(x_test)
Ypredicted = np.argmax(Ypredicted, axis = -1)
Y_true = np.argmax(y_test,axis = -1) 
create_confusion_matrix(Ypredicted,Y_true)



test_pred = model.predict(x_test)
# Convert predictions classes to one hot vectors 
test_pred_classes = np.argmax(test_pred,axis = 1) 
test_pred_classes = to_categorical(test_pred_classes, num_classes = 7)

from sklearn.metrics import classification_report
# Generate a classification report
testreport = classification_report(y_test, test_pred_classes, target_names=plot_labels)

print(testreport)



"""## DenseNet201"""



from keras.applications.densenet import DenseNet201
# load model
model4 = Sequential()
model4.add(DenseNet201(include_top = False,input_shape = (224,224,3),pooling = 'avg',weights = 'imagenet'))
model4.add(Flatten())
model4.add(Dense(1024, activation='relu'))
model4.add(Dropout(0.5))
model4.add(BatchNormalization())
model4.add(Dense(7, activation = "softmax"))
model4.summary()
batch_size = 16
epoch = 30
model4.compile(optimizer=keras.optimizers.RMSprop(lr = 2e-5),loss='binary_crossentropy',metrics=['accuracy'])

history = model4.fit(datagen.flow(x_train,y_train, batch_size = batch_size), epochs = epoch,
                              validation_data = (x_val, y_val), verbose = 1,
                                  steps_per_epoch=x_train.shape[0] // batch_size,
                                  callbacks=[cb_checkpointer])



_, test_acc = model4.evaluate(x_test, y_test, verbose=0)

test_acc

plot_model_history(history)

Ypredicted=model4.predict(x_test)
Ypredicted = np.argmax(Ypredicted, axis = 1)
Y_true = np.argmax(y_test,axis = 1)

create_confusion_matrix(Ypredicted,Y_true)

test_pred = model4.predict(x_test)
# Convert predictions classes to one hot vectors 
test_pred_classes = np.argmax(test_pred,axis = 1) 
test_pred_classes = to_categorical(test_pred_classes, num_classes = 7)
testreport = classification_report(y_test, test_pred_classes, target_names=plot_labels)

print(testreport)





"""## CNN model"""

num_classes = 7

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=(224,224,3)))
model.add(Conv2D(32,kernel_size=(3, 3), activation='relu',padding = 'Same',))
model.add(MaxPool2D(pool_size = (2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.40))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(optimizer=keras.optimizers.Adam(lr = 0.001),loss='binary_crossentropy',metrics=['accuracy'])

epochs = 30 
batch_size = 16

learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)


history = model.fit(datagen.flow(x_train,y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (x_val,y_val),
                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction])

_, test_acc = model.evaluate(x_test, y_test, verbose=0)

test_acc

plot_model_history(history)



Ypredicted=model.predict(x_test)
Ypredicted = np.argmax(Ypredicted, axis = 1)
Y_true = np.argmax(y_test,axis = 1)

create_confusion_matrix(Ypredicted,Y_true)

test_pred = model.predict(x_test)
# Convert predictions classes to one hot vectors 
test_pred_classes = np.argmax(test_pred,axis = 1) 
test_pred_classes = to_categorical(test_pred_classes, num_classes = 7)
testreport = classification_report(y_test, test_pred_classes, target_names=plot_labels)

print(testreport)

